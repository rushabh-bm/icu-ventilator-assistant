{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83872af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CSV or model/test data found. Running demo with synthetic example.\n",
      "Saved plots to: e:\\venti\\evaluation_plots\n",
      "Demo metrics: {'R2': 0.8610378944213931, 'MAE': 15.116888810886513, 'RMSE': 19.038105620423053}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation & Diagnostic Plots for Tidal Volume Prediction\n",
    "Saves plots to disk and displays metrics.\n",
    "\n",
    "Requirements:\n",
    "pip install pandas numpy matplotlib scikit-learn joblib\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import math\n",
    "\n",
    "# --------- USER CONFIG ----------\n",
    "# Option A: If you have a CSV that already contains true and predicted values\n",
    "# Set csv_path and column names appropriately\n",
    "csv_path = \"predictions.csv\"   # e.g. contains columns: 'y_true', 'y_pred'\n",
    "y_true_col = \"y_true\"\n",
    "y_pred_col = \"y_pred\"\n",
    "\n",
    "# Option B: If you have a saved model and test data (uncomment and edit)\n",
    "model_path = \"rf_model.joblib\"     # serialized scikit-learn model\n",
    "X_test_path = \"X_test.csv\"         # features used for prediction\n",
    "y_test_path = \"y_test.csv\"         # true target values (single column or dataframe)\n",
    "\n",
    "# Output folder for images\n",
    "out_dir = \"evaluation_plots\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "# ---------------------------------\n",
    "\n",
    "def load_from_csv(path, y_true_col, y_pred_col):\n",
    "    df = pd.read_csv(path)\n",
    "    if y_true_col not in df.columns or y_pred_col not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain columns '{y_true_col}' and '{y_pred_col}'. Columns found: {list(df.columns)}\")\n",
    "    return df[y_true_col].to_numpy(), df[y_pred_col].to_numpy(), df\n",
    "\n",
    "def load_from_model(model_path, X_test_path, y_test_path):\n",
    "    model = joblib.load(model_path)\n",
    "    X_test = pd.read_csv(X_test_path)\n",
    "    # if y_test in same file as X_test, change accordingly\n",
    "    y_test_df = pd.read_csv(y_test_path)\n",
    "    # assume y_test is single column\n",
    "    if y_test_df.shape[1] == 1:\n",
    "        y_test = y_test_df.iloc[:,0].to_numpy()\n",
    "    else:\n",
    "        # try column named 'y' or 'y_true' etc.\n",
    "        candidates = ['y', 'y_true', 'Actual', 'Actual_VT', 'true_vt']\n",
    "        found = None\n",
    "        for c in candidates:\n",
    "            if c in y_test_df.columns:\n",
    "                found = c\n",
    "                break\n",
    "        if found:\n",
    "            y_test = y_test_df[found].to_numpy()\n",
    "        else:\n",
    "            # take first column\n",
    "            y_test = y_test_df.iloc[:,0].to_numpy()\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_test, y_pred, X_test, model\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return {\"R2\": r2, \"MAE\": mae, \"RMSE\": rmse}\n",
    "\n",
    "def plot_diagnostics(y_true, y_pred, df_meta=None, out_dir=\"evaluation_plots\", prefix=\"eval\"):\n",
    "    metrics = compute_metrics(y_true, y_pred)\n",
    "    # Pred vs Actual scatter\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_true, y_pred, s=30, alpha=0.6)\n",
    "    minv = min(min(y_true), min(y_pred))\n",
    "    maxv = max(max(y_true), max(y_pred))\n",
    "    padding = (maxv - minv) * 0.05\n",
    "    plt.plot([minv-padding, maxv+padding], [minv-padding, maxv+padding], 'k--', linewidth=1)  # y=x line\n",
    "    plt.xlabel(\"Actual Tidal Volume (ml)\")\n",
    "    plt.ylabel(\"Predicted Tidal Volume (ml)\")\n",
    "    plt.title(\"Predicted vs Actual - Tidal Volume\")\n",
    "    # show metrics text box\n",
    "    textbox = '\\n'.join([f\"{k}: {v:.3f}\" if k!='MAE' and k!='RMSE' else f\"{k}: {v:.2f} ml\" \n",
    "                         for k,v in metrics.items()])\n",
    "    plt.gca().text(0.02, 0.98, textbox, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    plt.grid(alpha=0.3)\n",
    "    fname1 = os.path.join(out_dir, f\"{prefix}_pred_vs_actual.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname1, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Residuals histogram\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(residuals, bins=30, alpha=0.8)\n",
    "    plt.xlabel(\"Residuals (Actual - Predicted) [ml]\")\n",
    "    plt.title(\"Residuals Distribution\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    fname2 = os.path.join(out_dir, f\"{prefix}_residuals_hist.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname2, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Residuals vs Predicted\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.scatter(y_pred, residuals, s=25, alpha=0.6)\n",
    "    plt.axhline(0, color='k', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Tidal Volume (ml)\")\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted) [ml]\")\n",
    "    plt.title(\"Residuals vs Predicted\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    fname3 = os.path.join(out_dir, f\"{prefix}_residuals_vs_pred.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname3, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Error vs Feature (optional) - if df_meta present and has 'BMI' or 'Height', plot relationship\n",
    "    if df_meta is not None:\n",
    "        # try plotting error vs top few features if present\n",
    "        for col in [\"BMI\", \"Height\", \"Weight\", \"PaO2\", \"SpO2\"]:\n",
    "            if col in df_meta.columns:\n",
    "                plt.figure(figsize=(7,4))\n",
    "                plt.scatter(df_meta[col], residuals, s=25, alpha=0.6)\n",
    "                plt.axhline(0, color='k', linestyle='--')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel(\"Residual (ml)\")\n",
    "                plt.title(f\"Residuals vs {col}\")\n",
    "                plt.grid(alpha=0.3)\n",
    "                fname = os.path.join(out_dir, f\"{prefix}_residuals_vs_{col}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(fname, dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "    print(\"Saved plots to:\", os.path.abspath(out_dir))\n",
    "    return metrics, [fname1, fname2, fname3]\n",
    "\n",
    "def main():\n",
    "    # Try Option A first: CSV with y_true & y_pred\n",
    "    if os.path.exists(csv_path):\n",
    "        try:\n",
    "            y_true, y_pred, df = load_from_csv(csv_path, y_true_col, y_pred_col)\n",
    "            # If there are other columns in df, we can use for meta plots\n",
    "            metrics, files = plot_diagnostics(y_true, y_pred, df_meta=df, out_dir=out_dir, prefix=\"predcsv\")\n",
    "            print(\"Metrics (from CSV):\", metrics)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"CSV load failed:\", e)\n",
    "    # If CSV not available or failed, try Option B: load model & X_test\n",
    "    if os.path.exists(model_path) and os.path.exists(X_test_path) and os.path.exists(y_test_path):\n",
    "        try:\n",
    "            y_test, y_pred, X_test, model = load_from_model(model_path, X_test_path, y_test_path)\n",
    "            metrics, files = plot_diagnostics(y_test, y_pred, df_meta=X_test, out_dir=out_dir, prefix=\"from_model\")\n",
    "            print(\"Metrics (from model):\", metrics)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"Model-based evaluation failed:\", e)\n",
    "\n",
    "    # If neither option worked, show example using random data (demo)\n",
    "    print(\"No CSV or model/test data found. Running demo with synthetic example.\")\n",
    "    np.random.seed(0)\n",
    "    y_true = np.random.normal(loc=450, scale=50, size=200)  # e.g., actual vt ml\n",
    "    noise = np.random.normal(loc=0, scale=20, size=200)\n",
    "    y_pred = y_true + noise\n",
    "    metrics, files = plot_diagnostics(y_true, y_pred, df_meta=None, out_dir=out_dir, prefix=\"demo\")\n",
    "    print(\"Demo metrics:\", metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57eea79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CSV or model/test data found. Running demo with synthetic example.\n",
      "Saved plots to: e:\\venti\\evaluation_plots\n",
      "Demo metrics: {'R2': 0.8610378944213931, 'MAE': 15.116888810886513, 'RMSE': 19.038105620423053}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation & Diagnostic Plots for Tidal Volume Prediction\n",
    "Saves plots to disk and displays metrics.\n",
    "\n",
    "Requirements:\n",
    "pip install pandas numpy matplotlib scikit-learn joblib\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import math\n",
    "\n",
    "# --------- USER CONFIG ----------\n",
    "# Option A: If you have a CSV that already contains true and predicted values\n",
    "# Set csv_path and column names appropriately\n",
    "csv_path = \"predictions.csv\"   # e.g. contains columns: 'y_true', 'y_pred'\n",
    "y_true_col = \"y_true\"\n",
    "y_pred_col = \"y_pred\"\n",
    "\n",
    "# Option B: If you have a saved model and test data (uncomment and edit)\n",
    "model_path = \"rf_model.joblib\"     # serialized scikit-learn model\n",
    "X_test_path = \"X_test.csv\"         # features used for prediction\n",
    "y_test_path = \"y_test.csv\"         # true target values (single column or dataframe)\n",
    "\n",
    "# Output folder for images\n",
    "out_dir = \"evaluation_plots\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "# ---------------------------------\n",
    "\n",
    "def load_from_csv(path, y_true_col, y_pred_col):\n",
    "    df = pd.read_csv(path)\n",
    "    if y_true_col not in df.columns or y_pred_col not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain columns '{y_true_col}' and '{y_pred_col}'. Columns found: {list(df.columns)}\")\n",
    "    return df[y_true_col].to_numpy(), df[y_pred_col].to_numpy(), df\n",
    "\n",
    "def load_from_model(model_path, X_test_path, y_test_path):\n",
    "    model = joblib.load(model_path)\n",
    "    X_test = pd.read_csv(X_test_path)\n",
    "    # if y_test in same file as X_test, change accordingly\n",
    "    y_test_df = pd.read_csv(y_test_path)\n",
    "    # assume y_test is single column\n",
    "    if y_test_df.shape[1] == 1:\n",
    "        y_test = y_test_df.iloc[:,0].to_numpy()\n",
    "    else:\n",
    "        # try column named 'y' or 'y_true' etc.\n",
    "        candidates = ['y', 'y_true', 'Actual', 'Actual_VT', 'true_vt']\n",
    "        found = None\n",
    "        for c in candidates:\n",
    "            if c in y_test_df.columns:\n",
    "                found = c\n",
    "                break\n",
    "        if found:\n",
    "            y_test = y_test_df[found].to_numpy()\n",
    "        else:\n",
    "            # take first column\n",
    "            y_test = y_test_df.iloc[:,0].to_numpy()\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_test, y_pred, X_test, model\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return {\"R2\": r2, \"MAE\": mae, \"RMSE\": rmse}\n",
    "\n",
    "def plot_diagnostics(y_true, y_pred, df_meta=None, out_dir=\"evaluation_plots\", prefix=\"eval\"):\n",
    "    metrics = compute_metrics(y_true, y_pred)\n",
    "    # Pred vs Actual scatter\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_true, y_pred, s=30, alpha=0.6)\n",
    "    minv = min(min(y_true), min(y_pred))\n",
    "    maxv = max(max(y_true), max(y_pred))\n",
    "    padding = (maxv - minv) * 0.05\n",
    "    plt.plot([minv-padding, maxv+padding], [minv-padding, maxv+padding], 'k--', linewidth=1)  # y=x line\n",
    "    plt.xlabel(\"Actual Tidal Volume (ml)\")\n",
    "    plt.ylabel(\"Predicted Tidal Volume (ml)\")\n",
    "    plt.title(\"Predicted vs Actual - Tidal Volume\")\n",
    "    # show metrics text box\n",
    "    textbox = '\\n'.join([f\"{k}: {v:.3f}\" if k!='MAE' and k!='RMSE' else f\"{k}: {v:.2f} ml\" \n",
    "                         for k,v in metrics.items()])\n",
    "    plt.gca().text(0.02, 0.98, textbox, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    plt.grid(alpha=0.3)\n",
    "    fname1 = os.path.join(out_dir, f\"{prefix}_pred_vs_actual.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname1, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Residuals histogram\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(residuals, bins=30, alpha=0.8)\n",
    "    plt.xlabel(\"Residuals (Actual - Predicted) [ml]\")\n",
    "    plt.title(\"Residuals Distribution\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    fname2 = os.path.join(out_dir, f\"{prefix}_residuals_hist.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname2, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Residuals vs Predicted\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.scatter(y_pred, residuals, s=25, alpha=0.6)\n",
    "    plt.axhline(0, color='k', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Tidal Volume (ml)\")\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted) [ml]\")\n",
    "    plt.title(\"Residuals vs Predicted\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    fname3 = os.path.join(out_dir, f\"{prefix}_residuals_vs_pred.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname3, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Error vs Feature (optional) - if df_meta present and has 'BMI' or 'Height', plot relationship\n",
    "    if df_meta is not None:\n",
    "        # try plotting error vs top few features if present\n",
    "        for col in [\"BMI\", \"Height\", \"Weight\", \"PaO2\", \"SpO2\"]:\n",
    "            if col in df_meta.columns:\n",
    "                plt.figure(figsize=(7,4))\n",
    "                plt.scatter(df_meta[col], residuals, s=25, alpha=0.6)\n",
    "                plt.axhline(0, color='k', linestyle='--')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel(\"Residual (ml)\")\n",
    "                plt.title(f\"Residuals vs {col}\")\n",
    "                plt.grid(alpha=0.3)\n",
    "                fname = os.path.join(out_dir, f\"{prefix}_residuals_vs_{col}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(fname, dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "    print(\"Saved plots to:\", os.path.abspath(out_dir))\n",
    "    return metrics, [fname1, fname2, fname3]\n",
    "\n",
    "def main():\n",
    "    # Try Option A first: CSV with y_true & y_pred\n",
    "    if os.path.exists(csv_path):\n",
    "        try:\n",
    "            y_true, y_pred, df = load_from_csv(csv_path, y_true_col, y_pred_col)\n",
    "            # If there are other columns in df, we can use for meta plots\n",
    "            metrics, files = plot_diagnostics(y_true, y_pred, df_meta=df, out_dir=out_dir, prefix=\"predcsv\")\n",
    "            print(\"Metrics (from CSV):\", metrics)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"CSV load failed:\", e)\n",
    "    # If CSV not available or failed, try Option B: load model & X_test\n",
    "    if os.path.exists(model_path) and os.path.exists(X_test_path) and os.path.exists(y_test_path):\n",
    "        try:\n",
    "            y_test, y_pred, X_test, model = load_from_model(model_path, X_test_path, y_test_path)\n",
    "            metrics, files = plot_diagnostics(y_test, y_pred, df_meta=X_test, out_dir=out_dir, prefix=\"from_model\")\n",
    "            print(\"Metrics (from model):\", metrics)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"Model-based evaluation failed:\", e)\n",
    "\n",
    "    # If neither option worked, show example using random data (demo)\n",
    "    print(\"No CSV or model/test data found. Running demo with synthetic example.\")\n",
    "    np.random.seed(0)\n",
    "    y_true = np.random.normal(loc=450, scale=50, size=200)  # e.g., actual vt ml\n",
    "    noise = np.random.normal(loc=0, scale=20, size=200)\n",
    "    y_pred = y_true + noise\n",
    "    metrics, files = plot_diagnostics(y_true, y_pred, df_meta=None, out_dir=out_dir, prefix=\"demo\")\n",
    "    print(\"Demo metrics:\", metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea6d8779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to tv_predictor_model.pkl\n",
      "MAE: 10.3278 ml\n",
      "RMSE: 13.0555 ml\n",
      "R2: 0.9384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25804\\3260225920.py:163: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(shap_values, sample, show=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP summary saved.\n",
      "All plots and CSV saved in 'plots/' directory.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluate_and_plot.py\n",
    "Train RandomForest, compute MAE/RMSE/R2, save useful plots:\n",
    " - performance_metrics.png (MAE & R2 bar)\n",
    " - rmse_text.png (RMSE displayed)\n",
    " - predicted_vs_actual.png\n",
    " - residuals_plot.png\n",
    " - learning_curve.png\n",
    " - feature_importance.png\n",
    " - shap_summary.png (if SHAP available)\n",
    "Also saves model as tv_predictor_model.pkl\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- User settings ----------\n",
    "CSV_PATH = \"icu_simulated_data.csv\"\n",
    "TARGET_COL = \"TV_recommendation\"    # change if target column name differs\n",
    "MODEL_OUT = \"tv_predictor_model.pkl\"\n",
    "OUTDIR = \"plots\"\n",
    "RANDOM_STATE = 42\n",
    "# -----------------------------------\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET_COL}' not found in {CSV_PATH}\")\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Simple preprocessing: If there are non-numeric columns, try one-hot encode them\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 2) Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 3) Train model\n",
    "model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=RANDOM_STATE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, MODEL_OUT)\n",
    "print(f\"Model saved to {MODEL_OUT}\")\n",
    "\n",
    "# 4) Predictions & metrics\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MAE: {:.4f} ml\".format(mae))\n",
    "print(\"RMSE: {:.4f} ml\".format(rmse))\n",
    "print(\"R2: {:.4f}\".format(r2))\n",
    "\n",
    "# 5) Plot: MAE & R2 bar chart\n",
    "plt.figure(figsize=(6,4))\n",
    "metrics = ['MAE (ml)', 'R2 Score']\n",
    "values = [mae, r2]\n",
    "# To keep bar scale sensible, plot R2 on [0,1] and MAE as absolute; annotate values\n",
    "plt.bar(metrics, values)\n",
    "plt.title(\"Model Performance Metrics\")\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.02 * max(values), f\"{v:.3f}\", ha='center', fontsize=10)\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"performance_metrics.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 6) Plot: RMSE big text\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.axis('off')\n",
    "plt.text(0.5, 0.5, f\"RMSE = {rmse:.3f} ml\", horizontalalignment='center',\n",
    "         verticalalignment='center', fontsize=24, weight='bold')\n",
    "plt.savefig(os.path.join(OUTDIR, \"rmse_text.png\"), bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 7) Predicted vs Actual scatter with y=x line\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "minv = min(min(y_test), min(y_pred))\n",
    "maxv = max(max(y_test), max(y_pred))\n",
    "plt.plot([minv, maxv], [minv, maxv], 'k--', linewidth=1)  # y = x\n",
    "plt.xlabel(\"Actual TV_recommendation (ml)\")\n",
    "plt.ylabel(\"Predicted TV_recommendation (ml)\")\n",
    "plt.title(\"Predicted vs Actual\")\n",
    "plt.text(0.05, 0.95, f\"R² = {r2:.3f}\\nMAE = {mae:.3f} ml\\nRMSE = {rmse:.3f} ml\",\n",
    "         transform=plt.gca().transAxes, verticalalignment='top', bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.7))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"predicted_vs_actual.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 8) Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.hlines(0, xmin=min(y_pred), xmax=max(y_pred), linestyles='dashed', colors='k')\n",
    "plt.xlabel(\"Predicted TV_recommendation (ml)\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"residuals_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 9) Learning curve (uses cross-validation)\n",
    "cv = 5\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 5), random_state=RANDOM_STATE\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', label=f\"Cross-validation score (cv={cv})\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Score (R²)\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"learning_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 10) Feature importance (from RandomForest)\n",
    "try:\n",
    "    importances = model.feature_importances_\n",
    "    feat_names = X_train.columns\n",
    "    fi = pd.Series(importances, index=feat_names).sort_values(ascending=False)[:20]  # top 20\n",
    "    plt.figure(figsize=(8,6))\n",
    "    fi.plot(kind='barh')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.title(\"Top Feature Importances (RandomForest)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTDIR, \"feature_importance.png\"))\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(\"Feature importance plotting failed:\", e)\n",
    "\n",
    "# 11) Optional: SHAP summary plot (if SHAP is installed); saves shap_summary.png\n",
    "try:\n",
    "    import shap\n",
    "    # Use TreeExplainer for tree-based models\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    # Use sample for speed if dataset is large\n",
    "    sample = X_test.sample(min(100, len(X_test)), random_state=RANDOM_STATE)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "    # summary_plot writes to matplotlib; use show=False then save\n",
    "    shap.summary_plot(shap_values, sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTDIR, \"shap_summary.png\"))\n",
    "    plt.close()\n",
    "    print(\"SHAP summary saved.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP not available or SHAP plotting failed. Error:\", e)\n",
    "\n",
    "# 12) Save a small CSV comparing actual vs predicted for inspection\n",
    "df_compare = pd.DataFrame({\"actual\": y_test, \"predicted\": y_pred, \"residual\": residuals})\n",
    "df_compare.to_csv(os.path.join(OUTDIR, \"actual_vs_predicted.csv\"), index=False)\n",
    "\n",
    "print(f\"All plots and CSV saved in '{OUTDIR}/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01220691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
